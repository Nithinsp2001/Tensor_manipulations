1. What patterns do you observe in the training and validation accuracy curves?
Observing the training and validation accuracy curves during model training can reveal several patterns:

Converging Curves: If both training and validation accuracy increase and plateau, it indicates the model is learning well.

Diverging Curves: If the training accuracy increases while the validation accuracy stagnates or decreases, it could indicate overfitting, where the model performs well on the training data but poorly on unseen data.

Flat Curves: If both training and validation accuracy remain flat or very low, it may suggest underfitting, potentially due to an insufficient model architecture or poor feature representation.

Oscillations: Large oscillations in the curves might indicate an inappropriate learning rate or noisy data.

2. How can you use TensorBoard to detect overfitting?
TensorBoard provides tools to visualize and monitor model training, making it easier to detect overfitting:

Accuracy/ Loss Graphs: Compare training and validation accuracy (or loss). Overfitting is indicated when training accuracy continues to improve while validation accuracy declines or plateaus.

Histograms of Weights and Biases: Sudden large changes or instability in weights can hint at overfitting.

Learning Rate Visualization: If the learning rate remains constant and validation loss increases, it might contribute to overfitting.

Distribution Analysis: Use TensorBoard to monitor the distribution of activations or gradients. Overfitting can manifest as extreme values or lack of diversity in these distributions.

3. What happens when you increase the number of epochs?
Increasing the number of epochs affects model performance depending on the dataset and the model:

Improved Training: Initially, the model learns better patterns, increasing both training and validation accuracy.

Overfitting Risk: Beyond a certain point, the model may memorize the training data, leading to a divergence where training accuracy improves, but validation accuracy decreases.

Training Time: Increasing epochs also increases training time and computational cost, which may not always lead to better results.

Plateauing: Accuracy or loss may plateau, indicating that the model has learned as much as it can from the data.

Strategies to Handle Overfitting with Increased Epochs:
Use early stopping to halt training when validation accuracy stops improving.

Apply regularization techniques like dropout, L2 regularization, or data augmentation.

Monitor metrics using TensorBoard or similar tools.  
